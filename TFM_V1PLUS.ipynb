{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 El Notebook de Entrenamiento - Celdas\n",
        "\n",
        "Copia estas celdas exactamente en tu notebook de Colab. Cada celda est√° comentada explicando qu√© hace.\n",
        "\n",
        "#### CELDA 1: Verificar que tienes GPU"
      ],
      "metadata": {
        "id": "863rRk0-5mUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELDA 1: Verificar GPU\n",
        "# Ejecuta esto PRIMERO para confirmar que tienes GPU\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "\n",
        "# Verificar si hay GPU disponible\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"‚úÖ GPU detectada: {gpu_name}\")\n",
        "    print(f\"‚úÖ Memoria: {gpu_memory:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå NO HAY GPU - Ve a Entorno de ejecuci√≥n ‚Üí Cambiar tipo\")\n"
      ],
      "metadata": {
        "id": "lzIXgtem5qAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CELDA 2: Instalar dependencias"
      ],
      "metadata": {
        "id": "uKJOH_GJ5yKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELDA 2: Instalar dependencias\n",
        "# Unsloth hace el entrenamiento 2x m√°s r√°pido\n",
        "# Esta celda tarda ~3-5 minutos\n",
        "# ============================================\n",
        "\n",
        "# Instalar Unsloth (optimizado para Colab)\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Instalar dependencias adicionales\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "# Instalar utilidades\n",
        "!pip install datasets transformers huggingface_hub\n",
        "\n",
        "print(\"\\n‚úÖ Instalaci√≥n completada!\")\n"
      ],
      "metadata": {
        "id": "pkm7u2kZ6Kzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CELDA 3: Cargar el modelo"
      ],
      "metadata": {
        "id": "fwiyZP9B73ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELDA 3: Cargar el modelo Qwen2.5-Coder\n",
        "# Usamos Qwen porque NO requiere aprobaci√≥n (a diferencia de LLaMA)\n",
        "# ============================================\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Configuraci√≥n del modelo\n",
        "max_seq_length = 2048  # Longitud m√°xima de secuencia\n",
        "load_in_4bit = True    # Cuantizaci√≥n 4-bit (reduce memoria)\n",
        "\n",
        "# Cargar modelo pre-cuantizado (m√°s r√°pido)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-Coder-7B-bnb-4bit\",  # Modelo optimizado\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Modelo cargado correctamente!\")\n",
        "print(f\"Memoria GPU usada: {torch.cuda.memory_allocated()/1e9:.1f} GB\")\n"
      ],
      "metadata": {
        "id": "QYY8adKa779J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CELDA 4: Configurar LoRA para fine-tuning"
      ],
      "metadata": {
        "id": "EPWJh5vX9p_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELDA 4: Configurar LoRA\n",
        "# LoRA permite entrenar solo una peque√±a parte del modelo\n",
        "# Esto ahorra memoria y tiempo\n",
        "# ============================================\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Rank: 16 es buen balance calidad/velocidad\n",
        "    target_modules=[\n",
        "        \"q_proj\",   # Capa de Query\n",
        "        \"k_proj\",   # Capa de Key\n",
        "        \"v_proj\",   # Capa de Value\n",
        "        \"o_proj\",   # Capa de Output\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,      # Escala de LoRA\n",
        "    lora_dropout=0,     # Sin dropout (m√°s estable)\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Ahorra memoria\n",
        ")\n",
        "\n",
        "# Contar par√°metros entrenables\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\n‚úÖ LoRA configurado!\")\n",
        "print(f\"Par√°metros entrenables: {trainable:,} ({100*trainable/total:.2f}%)\")\n"
      ],
      "metadata": {
        "id": "og_soRZg9v-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CELDA 5: Preparar datos de CredData"
      ],
      "metadata": {
        "id": "xGxy7ZVa-C5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELDA 5: Preparar datos de entrenamiento\n",
        "# Carga CredData desde Google Drive y crea train_dataset\n",
        "# ============================================\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "from datasets import Dataset\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 1. Montar Google Drive\n",
        "# ---------------------------------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta donde tienes CredData (ajusta si es diferente)\n",
        "data_path = \"/content/drive/MyDrive/TFM/CredData/\"\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 2. Funci√≥n para formatear ejemplos\n",
        "# ---------------------------------------------\n",
        "def format_prompt(code_snippet: str, is_secret: bool) -> str:\n",
        "    \"\"\"\n",
        "    Formatea cada ejemplo en el prompt que el modelo aprender√°.\n",
        "\n",
        "    Args:\n",
        "        code_snippet: Fragmento de c√≥digo a analizar\n",
        "        is_secret: True si contiene secreto, False si es seguro\n",
        "\n",
        "    Returns:\n",
        "        Prompt formateado para entrenamiento\n",
        "    \"\"\"\n",
        "    label = \"SECRET\" if is_secret else \"SAFE\"\n",
        "    return f\"\"\"Analyze this code for leaked secrets:\n",
        "```\n",
        "{code_snippet}\n",
        "```\n",
        "\n",
        "Classification: {label}\"\"\"\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 3. Cargar y procesar CredData\n",
        "# ---------------------------------------------\n",
        "print(\"üìÇ Cargando datos de CredData...\")\n",
        "\n",
        "# CredData tiene estructura: data/meta/ con archivos CSV\n",
        "# Buscar archivos CSV disponibles\n",
        "csv_files = []\n",
        "for root, dirs, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            csv_files.append(os.path.join(root, file))\n",
        "\n",
        "print(f\"   Encontrados {len(csv_files)} archivos CSV\")\n",
        "\n",
        "# Cargar todos los CSVs y combinarlos\n",
        "all_data = []\n",
        "for csv_file in csv_files:  # ¬°Usar TODOS los archivos!\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file, on_error='warn')\n",
        "        # CredData t√≠picamente tiene columnas: 'LineStart', 'LineEnd', 'Category', etc.\n",
        "        # Ajusta seg√∫n la estructura real de tus CSVs\n",
        "        if 'Category' in df.columns or 'Value' in df.columns:\n",
        "            all_data.append(df)\n",
        "            print(f\"   ‚úì Cargado: {os.path.basename(csv_file)} ({len(df)} filas)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚úó Error en {csv_file}: {e}\")\n",
        "\n",
        "# Si CredData no tiene la estructura esperada, usar datos de ejemplo\n",
        "if not all_data:\n",
        "    print(\"\\n‚ö†Ô∏è No se encontraron CSVs con estructura esperada.\")\n",
        "    print(\"   Usando dataset de ejemplo para prueba inicial...\")\n",
        "\n",
        "    # Dataset de ejemplo para verificar que el pipeline funciona\n",
        "    example_data = [\n",
        "        {\"code\": \"API_KEY = 'sk-1234567890abcdef'\", \"is_secret\": True},\n",
        "        {\"code\": \"AWS_SECRET = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\", \"is_secret\": True},\n",
        "        {\"code\": \"password = 'admin123'\", \"is_secret\": True},\n",
        "        {\"code\": \"GITHUB_TOKEN = 'ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\", \"is_secret\": True},\n",
        "        {\"code\": \"def calculate_sum(a, b):\\n    return a + b\", \"is_secret\": False},\n",
        "        {\"code\": \"MAX_RETRIES = 3\", \"is_secret\": False},\n",
        "        {\"code\": \"user_name = input('Enter name: ')\", \"is_secret\": False},\n",
        "        {\"code\": \"DEBUG = True\", \"is_secret\": False},\n",
        "    ] * 50  # Repetir para tener m√°s ejemplos\n",
        "\n",
        "    texts = [format_prompt(d[\"code\"], d[\"is_secret\"]) for d in example_data]\n",
        "else:\n",
        "    # Procesar los datos reales de CredData\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    print(f\"\\nüìä Total de filas combinadas: {len(combined_df)}\")\n",
        "    print(f\"   Columnas: {list(combined_df.columns)}\")\n",
        "\n",
        "    # Adaptar seg√∫n las columnas reales de CredData\n",
        "    # T√≠picamente necesitas: el c√≥digo/l√≠nea y si es secreto o no\n",
        "    texts = []\n",
        "    for _, row in combined_df.iterrows():\n",
        "        # Ajusta estos nombres de columna seg√∫n tu CredData\n",
        "        code = str(row.get('Value', row.get('Line', row.get('Content', ''))))\n",
        "        is_secret = row.get('Category', '') != '' or row.get('IsSecret', True)\n",
        "        if code.strip():\n",
        "            texts.append(format_prompt(code, is_secret))\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 4. Crear el Dataset para Hugging Face\n",
        "# ---------------------------------------------\n",
        "train_dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "print(f\"\\n‚úÖ train_dataset creado correctamente!\")\n",
        "print(f\"   N√∫mero de ejemplos: {len(train_dataset)}\")\n",
        "print(f\"   Columnas: {train_dataset.column_names}\")\n",
        "print(f\"\\nüìù Ejemplo de prompt:\")\n",
        "print(\"-\" * 50)\n",
        "print(train_dataset[0][\"text\"])\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "deuAWsEYAPPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CELDA 6: Entrenar el modelo"
      ],
      "metadata": {
        "id": "QtFfmHrI-uyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELDA 6: Entrenamiento\n",
        "# Requiere: model, tokenizer, train_dataset, max_seq_length\n",
        "# Duraci√≥n estimada: ~1-2 horas en T4\n",
        "# ============================================\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Verificar que train_dataset existe\n",
        "if 'train_dataset' not in dir():\n",
        "    raise NameError(\"‚ùå train_dataset no definido. Ejecuta la Celda 5 primero.\")\n",
        "\n",
        "print(f\"üìä Entrenando con {len(train_dataset)} ejemplos...\")\n",
        "\n",
        "# Configuraci√≥n del entrenamiento\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    dataset_text_field=\"text\",        # Columna con los prompts\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,   # Batch peque√±o para T4 (15GB VRAM)\n",
        "        gradient_accumulation_steps=8,   # Batch efectivo = 2 * 8 = 16\n",
        "        warmup_steps=10,\n",
        "        max_steps=500,                   # Ajustar seg√∫n tama√±o de datos\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,                       # Precisi√≥n mixta para ahorrar memoria\n",
        "        logging_steps=10,\n",
        "        output_dir=\"outputs\",\n",
        "        save_steps=100,\n",
        "        optim=\"adamw_8bit\",              # Optimizador eficiente en memoria\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Entrenar\n",
        "print(\"üöÄ Iniciando entrenamiento...\")\n",
        "trainer.train()\n",
        "print(\"\\n‚úÖ Entrenamiento completado!\")"
      ],
      "metadata": {
        "id": "ZU5EtLpCAdd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CELDA 7: Guardar el modelo entrenado"
      ],
      "metadata": {
        "id": "06c8gqtzJsh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELDA 7: Guardar modelo en Google Drive\n",
        "# MUY IMPORTANTE: Si no guardas, perder√°s todo al cerrar Colab\n",
        "# ============================================\n",
        "\n",
        "# Directorio en tu Google Drive\n",
        "save_path = \"/content/drive/MyDrive/TFM/models/secret-detector-v1\"\n",
        "\n",
        "# Guardar los adaptadores LoRA (son peque√±os, ~100MB)\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Modelo guardado en: {save_path}\")\n",
        "print(\"Archivos guardados:\")\n",
        "!ls -lh {save_path}\n"
      ],
      "metadata": {
        "id": "W7XBW0LiJvFD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}